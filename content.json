[{"title":"分布式事务","date":"2017-03-24T07:17:06.000Z","path":"2017/03/24/分布式事务/","text":"什么是分布式事务由多个本地事务组成一个分布式事务，有发起者和参与者，需要和单机事务表现的行为一样。 CAP 理论 C （Consistency） 一致性 A （AVALIABLE） 好的响应性能 P （Partition tolerance） 可靠性, 分区容忍性， 允许服务器宕机 BASEBASE模型反ACID模型，完全不同ACID模型，牺牲高一致性，获得可用性或可靠性： Basically Available基本可用。支持分区失败(e.g. sharding碎片划分数据库) Soft state软状态 状态可以有一段时间不同步，异步。 Eventually consistent最终一致，最终数据是一致的就可以了，而不是时时高一致。 单机事务 a（atomic） 原子性： 事务中的多个操作要么全部执行，要么全部不执行。 c （consistent）一致性： 事务执行前后，不管成功与否。整个数据库的完整性，一致性不受破坏。 i （isolation）隔离性： 多个事务间的操作互不影响。 d （duration）持久性： 事务一但confirm， 变更会保留到磁盘上。 数据库隔离级别： 读未提交可能会出现脏读，不可重复读，幻读 读提交可能会出现不可重复读幻读 。 读的记录，可能会被修改，第二次读不准。 可重复读可能会出现幻读。 可能会读到新插入的记录 串行 脏读： 读到其它事务未提交的数据不可重复读： 事务中看见的数据， 可能被另一个事务修改，导致第二次看，与第一次看到的不同。幻读： 事务中会读到其它事务新增的数据。 分布式事务类型 XA 两阶段 由事务管理器，资源管理器组成。事务管理器作为协调器，调度本地资源的提交与回滚。分为准备阶段，执行阶段。 image 缺点：性能不理想， mysql， nosql 等不支持。 消息事务 + 最终一致性 image A系统向消息中间件发送一条预备消息 消息中间件保存预备消息并返回成功 A执行本地事务 A发送提交消息给消息中间件 通过以上4步完成了一个消息事务。对于以上的4个步骤，每个步骤都可能产生错误，下面一一分析： 步骤一出错，则整个事务失败，不会执行A的本地操作 步骤二出错，则整个事务失败，不会执行A的本地操作 步骤三出错，这时候需要回滚预备消息，怎么回滚？答案是A系统实现一个消息中间件的回调接口，消息中间件会去不断执行回调接口，检查A事务执行是否执行成功，如果失败则回滚预备消息 步骤四出错，这时候A的本地事务是成功的，那么消息中间件要回滚A吗？答案是不需要，其实通过回调接口，消息中间件能够检查到A执行成功了，这时候其实不需要A发提交消息了，消息中间件可以自己对消息进行提交，从而完成整个消息事务将分布式事务 拆分为（消息事务+ 本地事务a），和本地事务b。本地事务b由消息驱动。 虽然上面的方案能够完成A和B的操作，但是A和B并不是严格一致的，而是最终一致的，我们在这里牺牲了一致性，换来了性能的大幅度提升。当然，这种玩法也是有风险的，如果B一直执行不成功，那么一致性会被破坏，具体要不要玩，还是得看业务能够承担多少风险。 TCC所谓的TCC编程模式，也是两阶段提交的一个变种。TCC提供了一个编程框架，将整个业务逻辑分为三块：Try、Confirm和Cancel三个操作。以在线下单为例，Try阶段会去扣库存，Confirm阶段则是去更新订单状态，如果更新订单失败，则进入Cancel阶段，会去恢复库存。总之，TCC就是通过代码人为实现了两阶段提交，不同的业务场景所写的代码都不一样，复杂度也不一样，因此，这种模式并不能很好地被复用。 参考1 参考2 参考3 参考4","tags":[{"name":"java","slug":"java","permalink":"http://boolbit.info/tags/java/"},{"name":"分布式","slug":"分布式","permalink":"http://boolbit.info/tags/分布式/"},{"name":"dubbo","slug":"dubbo","permalink":"http://boolbit.info/tags/dubbo/"},{"name":"微服务","slug":"微服务","permalink":"http://boolbit.info/tags/微服务/"}]},{"title":"jmeter+jprofiler性能压测","date":"2017-02-03T09:25:12.000Z","path":"2017/02/03/jmeter-jprofiler性能压测/","text":"jmeterjmeter是一款开源的java编写的压测工具，非常轻量级，win平台下载运行bin目录下的jmeter.bat即可启动，支持web请求，jdbc，java接口等方面的压测。 主要组件，配置介绍 1、Test Plan (测试计划) 用来描述一个性能测试，包含与本次性能测试所有相关的功能。也就说本的性能测试的所有内容是于基于一个计划的。 2、Threads （Users）线程 虽然有三个添加线程组的选项，名字不一样， 创建之后，其界面是完全一样的。之前的版本只有一个线程组的名字。现在多一个setUp theread Group 与terDown Thread Group setup thread group 一种特殊类型的ThreadGroup的，可用于执行预测试操作。这些线程的行为完全像一个正常的线程组元件。不同的是，这些类型的线程执行测试前进行定期线程组的执行。 可用于执行预测试操作,每次线程执行均执行一次。 teardown thread group. 一种特殊类型的ThreadGroup的，可用于执行测试后动作。这些线程的行为完全像一个正常的线程组元件。不同的是，这些类型的线程执行测试结束后执行定期的线程组。 可用于执行测试后动作,每次线程执行均执行一次。 thread group(线程组). 这个就是我们通常添加运行的线程。通俗的讲一个线程组,，可以看做一个虚拟用户组，线程组中的每个线程都可以理解为一个虚拟用户。线程组中包含的线程数量在测试执行过程中是不会发生改变的。 12345线程数：这里选择5Ramp-Up Period：单位是秒，默认时间是1秒。它指定了启动所有线程所花费的时间，比如，当前的设定表示“在5秒内启动5个线程，每个线程的间隔时间为1秒”。如果你需要Jmeter立即启动所有线程，将此设定为0即可循环次数：表示每个线程执行多少次请求。 3、测试片段（Test Fragment） 测试片段元素是控制器上的一个种特殊的线程组，它在测试树上与线程组处于一个层级。它与线程组有所不同，因为它不被执行，除非它是一个模块控制器或者是被控制器所引用时才会被执行。 控制器 JMeter有两种类型的控制器：取样器（sample）和逻辑控制器（Logic Controller），用这些原件来驱动处理一个测试。 4、取样器（Sampler） 取样器（Sampler）是性能测试中向服务器发送请求，记录响应信息，记录响应时间的最小单元，JMeter 原生支持多种不同的sampler ， 如 HTTP Request Sampler 、 FTP Request Sampler 、TCP Request Sampler 、 JDBC Request Sampler 等，每一种不同类型的 sampler 可以根据设置的参数向服务器发出不同类型的请求。 在Jmeter的所有Sampler中，Java Request Sampler与BeanShell Requst Sampler是两种特殊的可定制的Sampler. 5、逻辑控制器（Logic Controller） 逻辑控制器，包括两类无件，一类是用于控制test plan 中 sampler 节点发送请求的逻辑顺序的控制器，常用的有 如果（If）控制器 、 switch Controller 、Runtime Controller、循环控制器等。另一类是用来组织可控制 sampler 来节点的， 如 事务控制器、吞吐量控制器。 6、配置元件（Config Element） 配置元件（config element）用于提供对静态数据配置的支持。CSV Data Set config 可以将本地数据文件形成数据池 （Data Pool），而对应于HTTP Request Sampler和 TCP Request Sampler等类型的配制无件则可以修改 Sampler的默认数据。 7、定时器（Timer） 定时器（Timer）用于操作之间设置等待时间，等待时间是性能测试中常用的控制客户端QPS的手段。类似于LoadRunner里面的“思考时间”。 JMeter 定义了Bean Shell Timer、Constant Throughput Timer、固定定时器等不同类型的Timer。 8、前置处理器（Per Processors） 前置处理器用于在实际的请求发出之前对即将发出的请求进行特殊处理。例如，HTTP URL重写修复符则可以实现URL重写，当RUL中有sessionID 一类的session信息时，可以通过该处理器填充发出请求的实际的sessionID 。 9、后置处理器（Post Processors） 后置处理器是用于对Sampler 发出请求后得到的服务器响应进行处理。一般用来提取响应中的特定数据（类似LoadRunner测试工具中的关联概念）。例如，XPath Extractor 则可以用于提取响应数据中通过给定XPath 值获得的数据;正则表达式提取器，则可以提取响应数据中通过正则表达式获得的数据。 10、断言（Assertions） 断言用于检查测试中得到的相应数据等是否符合预期，断言一般用来设置检查点，用以保证性能测试过程中的数据交互是否与预期一致。 11、监听器（Listener） 这个监听器可不是用来监听系统资源的元件。它是用来对测试结果数据进行处理和可视化展示的一系列元件。 图形结果、查看结果树、聚合报告、用表格察看结果都是我们经常用到的元件。 使用badboy录制jmeter执行计划脚本jprofilerjprofiler是一个性能监控分析工具，支持cpu，内存，java方法调用栈消耗等功能，能够连接本地或者远程的java程序。 下载9.2版本，输入注册码安装后，在开始中心点击new remote integration，前三步看自己情况选择第四部选择第二个选项，输入远程地址，jprofiler目录地址，端口，结束。 按照perform modifications 阶段的提示，将本地指定目录的config.xml目录拷贝到远程jprofiler的主目录下. 修改服务器java应用所在的tomcat bin目录下，修改startup.sh 在ｅｘｅｃ最后执行前添加CATALINA_OPTS=”-agentpath:/mnt/server/jprofiler9/bin/linux-x64/libjprofilerti.so=port=8849,nowait $CATALINA_OPTS”export CATALINA_OPTS 服务器启动tomcat java应用，jprofiler attach上去即可，看到数据. 参考","tags":[{"name":"性能压测","slug":"性能压测","permalink":"http://boolbit.info/tags/性能压测/"},{"name":"jmeter","slug":"jmeter","permalink":"http://boolbit.info/tags/jmeter/"},{"name":"jprofiler","slug":"jprofiler","permalink":"http://boolbit.info/tags/jprofiler/"}]},{"title":"twitter分布式id生成器","date":"2016-12-06T08:13:50.000Z","path":"2016/12/06/twitter分布式id生成器/","text":"在查阅spring多版本控制方案的时候意外发现了以前面试的时候经常被问及的一个问题，分布式id生成器，记录一下。 概述分布式系统中，有一些需要使用全局唯一ID的场景，这种时候为了防止ID冲突可以使用36位的UUID，但是UUID有一些缺点，首先他相对比较长，另外UUID一般是无序的。 有些时候我们希望能使用一种简单一些的ID，并且希望ID能够按照时间有序生成。 而twitter的snowflake解决了这种需求，最初Twitter把存储系统从MySQL迁移到Cassandra，因为Cassandra没有顺序ID生成机制，所以开发了这样一套全局唯一ID生成服务。 结构snowflake的结构如下(每部分用-分开): 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 第一位为未使用，接下来的41位为毫秒级时间(41位的长度可以使用69年)，然后是5位datacenterId和5位workerId(10位的长度最多支持部署1024个节点） ，最后12位是毫秒内的计数（12位的计数顺序号支持每个节点每毫秒产生4096个ID序号） 一共加起来刚好64位，为一个Long型。(转换成字符串长度为18) snowflake生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和workerId作区分），并且效率较高。据说：snowflake每秒能够产生26万个ID。 java代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class IdWorker&#123; private final long workerId; private final static long twepoch = 1361753741828L; private long sequence = 0L; private final static long workerIdBits = 4L; public final static long maxWorkerId = -1L ^ -1L &lt;&lt; workerIdBits; private final static long sequenceBits = 10L; private final static long workerIdShift = sequenceBits; private final static long timestampLeftShift = sequenceBits + workerIdBits; public final static long sequenceMask = -1L ^ -1L &lt;&lt; sequenceBits; private long lastTimestamp = -1L; public IdWorker(final long workerId) &#123; super(); if (workerId &gt; this.maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format( \"worker Id can't be greater than %d or less than 0\", this.maxWorkerId)); &#125; this.workerId = workerId; &#125; public synchronized long nextId() &#123; long timestamp = this.timeGen(); if (this.lastTimestamp == timestamp) &#123; this.sequence = (this.sequence + 1) &amp; this.sequenceMask; if (this.sequence == 0) &#123; System.out.println(\"###########\" + sequenceMask); timestamp = this.tilNextMillis(this.lastTimestamp); &#125; &#125; else &#123; this.sequence = 0; &#125; if (timestamp &lt; this.lastTimestamp) &#123; try &#123; throw new Exception( String.format( \"Clock moved backwards. Refusing to generate id for %d milliseconds\", this.lastTimestamp - timestamp)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; this.lastTimestamp = timestamp; long nextId = ((timestamp - twepoch &lt;&lt; timestampLeftShift)) | (this.workerId &lt;&lt; this.workerIdShift) | (this.sequence);// System.out.println(\"timestamp:\" + timestamp + \",timestampLeftShift:\"// + timestampLeftShift + \",nextId:\" + nextId + \",workerId:\"// + workerId + \",sequence:\" + sequence); return nextId; &#125; private long tilNextMillis(final long lastTimestamp) &#123; long timestamp = this.timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = this.timeGen(); &#125; return timestamp; &#125; private long timeGen() &#123; return System.currentTimeMillis(); &#125; public static void main(String[] args)&#123; IdWorker worker2 = new IdWorker(2); System.out.println(worker2.nextId()); &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"http://boolbit.info/tags/java/"},{"name":"distribute","slug":"distribute","permalink":"http://boolbit.info/tags/distribute/"}]},{"title":"服务器cpu占用过高问题","date":"2016-12-05T07:35:50.000Z","path":"2016/12/05/服务器cpu负载过高与tcp/","text":"排查java程序之前公司网页项目有两台服务器，出现过cpu负载过高的情况，阿里云频繁报警。一开始以为是程序代码问题，通过top命令观察，java进程的负载比较平均都是比较低的，拿了一个进程按以下步骤查看： top -H -p pid 找到cpu负载比较高的线程，将其id转换成16进制。 执行jstack -l pid &gt; /tmp/out.txt, 获取dump文件，查询id所在线程，并不是gc线程，也不是程序线程，而是tomcat线程，这种线程比较多，怀疑是大量这种线程导致cpu占用过高了。 之后在一台正常的服务器上比较了下，tomcat线程数差不多，故排除是程序代码问题，因此极有可能是服务器硬件或者网络出现了异常。 排查网络情况：执行 netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39; 发现大量的time_wait 端口， 可能是因为服务器频繁释放链接，导致新的链接创建不了。time_wait 阶段是指主动关闭链接的一方在四次握手最后一个阶段，还会等待2个报文最大生存时间（msl）默认是120s，才会关闭链接. 修改系统的/etc/sysctl.conf配置来减少TIME_WAIT的tcp连接：123456vi /etc/sysctl.confnet.ipv4.tcp_syncookies = 1 （表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭）net.ipv4.tcp_tw_reuse = 1 （表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭）net.ipv4.tcp_tw_recycle = 1 （表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。）net.ipv4.tcp_fin_timeout = 30 （修改系統默认的TIMEOUT时间） 执行/sbin/sysctl -p让参数生效. 等待一段时间查看网络情况，发现time_wait状态变少，查看cpu占用情况，也恢复正常。猜测linux每次找到一个随机端口，还是需要遍历一遍bound ports的吧，这必然需要一些CPU时间，会导致cpu负载变高。 详解tcptcp属于osi七层模型中的第四层传输层， ip属于第三层网络层，arp属于第二层数据链路层。第二层上的数据叫做frame，第三层叫做package，第四层叫做segment。 tcp 三次握手, 四次握手 TCP头格式 需要注意的几点 sequence number 报文的序号，解决乱序的问题 acknowledgement Number 解决可靠传输的问题，不丢包 sliding window 解决用于流量控制的 tcp flag 操作tcp的状态机，如ACK，PUSH，SYN，FIN, RESET 状态机变化 三次握手连接： 客户端 CLOSED -&gt; SYN_SEND -&gt; ESTABLISHED 服务端 CLOSED -&gt; SYN_RECEIVED -&gt; ESTABLISHED 四次握手断开： 客户端 ESTABLISHED -&gt; FIN_CLOSED1 -&gt; FINCLOSED2 -&gt; TIME_WAIT -&gt; CLOSED 服务端 ESTABLISHED -&gt; SYN_RECEIVED -&gt; CLOSE_WAIT -&gt; ACK_WAIT -&gt; CLOSED 为什么会有连接需要三次握手和断开需要四次握手？ 三次握手：双方同步sequence number的初始值，所以叫做syn （synchronize sequence number）， 以保证数据的传输顺序的一致。 四次握手：可以看做两次断开过程。 需要主要的几个点: 客户端发出SYN后挂掉。试想一下，如果server端接到了clien发的SYN后回了SYN-ACK后client掉线了，server端没有收到client回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，server端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻售，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s，TCP才会把断开这个连接。 syn flood 攻击恶意攻击者可以发出SYN后马上挂掉，会导致大量的端口资源，会导致syn连接队列耗尽。linux提供了一个tcp_syncookies参数应对，当syn队列满了之后，linux会根据源，目标端口构，时间戳打造出一个特别的Sequence Number发回去（又叫cookie），如果是攻击者则不会有响应，如果是正常连接，则会把这个 SYN Cookie发回来，然后服务端可以通过cookie建连接（即使你不在SYN队列中）,对于正常的请求，你应该调整三个TCP参数可供你选择，第一个是：tcp_synack_retries 可以用他来减少重试次数；第二个是：tcp_max_syn_backlog，可以增大SYN连接数；第三个是：tcp_abort_on_overflow 处理不过来干脆就直接拒绝连接了。 time_wait我们注意到，在TCP的状态图中，从TIME_WAIT状态到CLOSED状态，有一个超时设置，这个超时设置是 2*MSL（RFC793定义了MSL为2分钟，Linux设置成了30s）为什么要这有TIME_WAIT？为什么不直接给转成CLOSED状态呢？主要有两个原因：1）TIME_WAIT确保有足够的时间让对端收到了ACK，如果被动关闭的那方没有收到Ack，就会触发被动端重发Fin，一来一去正好2个MSL，2）有足够的时间让这个连接不会跟后面的连接混在一起（你要知道，有些自做主张的路由器会缓存IP数据包，如果连接被重用了，那么这些延迟收到的包就有可能会跟新连接混在一起）。 time_wait 数量太多可以配置两个参数tcp_tw_reuse、tcp_tw_recycle这两个参数默认值都是被关闭的，后者recyle比前者resue更为激进，resue要温柔一些。另外，如果使用tcp_tw_reuse，必需设置tcp_timestamps=1，否则无效. 关于tcp_tw_reuse。官方文档上说tcp_tw_reuse 加上tcp_timestamps（又叫PAWS, for Protection Against Wrapped Sequence Numbers）可以保证协议的角度上的安全，但是你需要tcp_timestamps在两边都被打开（你可以读一下tcp_twsk_unique的源码 ）。我个人估计还是有一些场景会有问题。 关于tcp_tw_recycle。如果是tcp_tw_recycle被打开了话，会假设对端开启了tcp_timestamps，然后会去比较时间戳，如果时间戳变大了，就可以重用。但是，如果对端是一个NAT网络的话（如：一个公司只用一个IP出公网）或是对端的IP被另一台重用了，这个事就复杂了。建链接的SYN可能就被直接丢掉了（你可能会看到connection time out的错误）（如果你想观摩一下Linux的内核代码，请参看源码 tcp_timewait_state_process）。 关于tcp_max_tw_buckets。这个是控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow），官网文档说这个参数是用来对抗DDoS攻击的。也说的默认值180000并不小。这个还是需要根据实际情况考虑。 TCP重传机制TCP要保证所有的数据包都可以到达，所以，必需要有重传机制。 注意，接收端给发送端的Ack确认只会确认最后一个连续的包，比如，发送端发了1,2,3,4,5一共五份数据，接收端收到了1，2于是回ack 3，然后收到了4（注意此时3没收到），此时的TCP会怎么办？我们要知道，因为正如前面所说的，SeqNum和Ack是以字节数为单位，所以ack的时候，不能跳着确认，只能确认最大的连续收到的包，不然，发送端就以为之前的都收到了。 超时重传机制一种是不回ack，死等3，当发送方发现收不到3的ack超时后，会重传3。一旦接收方收到3后，会ack 回 4——意味着3和4都收到了。 但是，这种方式会有比较严重的问题，那就是因为要死等3，所以会导致4和5即便已经收到了，而发送方也完全不知道发生了什么事，因为没有收到Ack，所以，发送方可能会悲观地认为也丢了，所以有可能也会导致4和5的重传。 对此有两种选择： 一种是仅重传timeout的包。也就是第3份数据。 另一种是重传timeout后所有的数据，也就是第3，4，5这三份数据。这两种方式有好也有不好。第一种会节省带宽，但是慢，第二种会快一点，但是会浪费带宽，也可能会有无用功。但总体来说都不好。因为都在等timeout，timeout可能会很长（在下篇会说TCP是怎么动态地计算出timeout的） 快速重传机制于是，TCP引入了一种叫Fast Retransmit 的算法，不以时间驱动，而以数据驱动重传。也就是说，如果，包没有连续到达，就ack最后那个可能被丢了的包，如果发送方连续收到3次相同的ack，就重传。Fast Retransmit的好处是不用等timeout了再重传。 比如：如果发送方发出了1，2，3，4，5份数据，第一份先到送了，于是就ack回2，结果2因为某些原因没收到，3到达了，于是还是ack回2，后面的4和5都到了，但是还是ack回2，因为2还是没有收到，于是发送端收到了三个ack=2的确认，知道了2还没有到，于是就马上重转2。然后，接收端收到了2，此时因为3，4，5都收到了，于是ack回6。示意图如下： Fast Retransmit只解决了一个问题，就是timeout的问题，它依然面临一个艰难的选择，就是，是重传之前的一个还是重传所有的问题。对于上面的示例来说，是重传#2呢还是重传#2，#3，#4，#5呢？因为发送端并不清楚这连续的3个ack(2)是谁传回来的？也许发送端发了20份数据，是#6，#10，#20传来的呢。这样，发送端很有可能要重传从2到20的这堆数据（这就是某些TCP的实际的实现）。可见，这是一把双刃剑。 参考 返回的这个结构里再加一个tname用户点了导航条的时候, 本地可以记一条记录，点击结构：{ target: tname (str) action: 2（str） time: 记录时间 （str） gpm：str （来源信息导航条暂时没有啊吧）} 到了一定记录数或者时间可以发送过来 /data/track 这个接口 params中是 trackList：[点击结构]","tags":[{"name":"tcp","slug":"tcp","permalink":"http://boolbit.info/tags/tcp/"},{"name":"cpu","slug":"cpu","permalink":"http://boolbit.info/tags/cpu/"}]},{"title":"hexo与github集成搭建blog","date":"2016-09-06T01:23:17.636Z","path":"2016/09/06/hexo与github集成搭建blog/","text":"之前在csdn，博客园上写博客，想着一方面总结学过的知识，另一方面可以提高自己的写作总结能力。很多知识点理解运用并不难，但是把逻辑清楚的讲解给别人听，还是需要一定的技巧和经验。Hexo是一款开源的Node.js静态博客框架，支持markdown语法，主题很丰富。之前在阿里云上搭过，可惜写了几篇没有坚持下来。痛下心痛决定结合github pages重新开始… 配置环境 git &amp; github （将页面推送到github pages，公司，家中分布式开发。。） nodejs (用于安装HEXO) markdown编辑器 （使用sublimes3 + MarkdownEditing+OmniMarkupPreviewer） hexo安装1234$ npm install -g hexo-cli$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 安装完成后目录如下： .├── _config.yml(网站全局配置信息，例如标题，关键词，配置文章、目录基本信息，选择主题，配置deploy信息等)├── package.json （应用程序信息）├── scaffolds（文章模板文件夹）├── source （资源文件夹，markdown和html文件会被生成到public文件夹，其它会被拷贝过去）| └── _posts （为一个页面目录，存放默认发表的文章）└── themes （存放主题插件和配置） 基本命令 newhexo new &lt;layout&gt; pageName新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，需用引号括起来。 server本地启动nodejs服务器，可通过url访问 generate生成静态页面和css等 publish生成草稿 （hexo –draft 显示草稿） clean清除缓存文件 (db.json) 和已生成的静态文件 (public)。 deploy部署public目录下的文件到配置的对应目的地（ 参数-g 首先执行generate页面） 自定义主题选择一个心仪的主题,进入主题github主页，按照教程进行相应配置即可。也可在此基础上进行开发。 部署到github编辑_config.yml文件 1234deploy: type: git repo: https://github.com/githubusername/githubusername.github.io.git （需要一个 用户名.github.io 为名称的github项目，用于deploy generate后的public目录。） branch: master 执行 npm install hexo-deployer-git --save 此外需要在github上配置了本机的ssh key。 123hexo new &quot;page&quot; 创建一片新文章hexo cleanhexo deploy -g 浏览器访问githubusername.github.io即可看到相应的页面。 绑定域名","tags":[{"name":"hexo","slug":"hexo","permalink":"http://boolbit.info/tags/hexo/"},{"name":"git","slug":"git","permalink":"http://boolbit.info/tags/git/"},{"name":"blog","slug":"blog","permalink":"http://boolbit.info/tags/blog/"}]}]